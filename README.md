# ðŸ”­ Stellar Classify Pro: End-to-End Stellar Object Classification ðŸŒŸ

[![Python Version](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE) 
[![Open in Streamlit](https://img.shields.io/badge/Open%20in%20Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://abdulrahmansami-exe-stellar-classification-capstone.streamlit.app)

**Classify the cosmos!** This project provides a complete machine learning pipeline to categorize celestial objects (Stars â­, Galaxies ðŸŒŒ, Quasars âœ¨) using data from the Sloan Digital Sky Survey (SDSS). It covers the entire workflow: data ingestion, cleaning, feature engineering, model training & comparison, deployment via an interactive Streamlit app, and in-depth model interpretability using SHAP.

---

### ðŸŒ Live Demo

**[ðŸ”­ Try the Streamlit App](https://abdulrahmansami-exe-stellar-classification-capstone.streamlit.app)**

![Streamlit App Demo](assets/streamlit_demo.gif)

---

## ðŸŽ¯ Project Goal

To build a robust, reproducible, and interpretable end-to-end pipeline for classifying stellar objects, deployable as an easy-to-use web application.

---
## ðŸ““ Notebooks

- **[EDA Notebook](notebooks/EDA.ipynb)** - Exploratory Data Analysis
- **[Complete Pipeline Notebook](https://nbviewer.org/github/abdulrahman-exe/Stellar_Classification_Capstone/blob/main/notebooks/Stellar_Classification_Capstone_Complete.ipynb)** - Full end-to-end workflow *(View on nbviewer)*


---
## ðŸš€ Key Features

* **ðŸŒŒ Data Processing:** Automated pipeline for loading, validating, cleaning (handling `-9999` values), engineering new features (color indices, magnitude stats), and splitting SDSS data while preventing data leakage.
* **âš–ï¸ Class Imbalance Handling:** Utilizes SMOTE within the pipeline to address the uneven distribution of object classes.
* **ðŸ§  Model Training & Selection:** Trains and rigorously evaluates multiple classifiers (Random Forest, XGBoost, Gradient Boosting, SVM, Logistic Regression) using metrics like Accuracy, F1-Score, and ROC AUC. Automatically selects and saves the best-performing model.
* **ðŸ’¡ Interpretability:** Employs **SHAP** (SHapley Additive exPlanations) for both global feature importance and local (per-prediction) explanations via waterfall plots.
* **ðŸ–¥ï¸ Interactive Web App:** A Streamlit application offering:
  * Real-time classification using intuitive sliders.
  * Exploration of the processed test dataset with visualizations.
  * Dashboard comparing the performance of trained models.
  * Interactive SHAP explanations for selected test samples.
* **âœ… Reproducibility:** Includes `requirements.txt` and clear setup instructions.

---

## ðŸ› ï¸ Tech Stack

* **Core:** Python 3.11+, Pandas, NumPy
* **ML Pipeline:** Scikit-learn, Imbalanced-learn, XGBoost
* **Interpretability:** SHAP
* **Web App & Viz:** Streamlit, Plotly, Matplotlib, Seaborn
* **Persistence:** Joblib

---

## âš™ï¸ Setup & Usage

1. **Clone the Repository:**

   ```bash
   git clone [https://github.com/abdulrahmansami_exe/Stellar_Classification_Capstone.git](https://github.com/abdulrahmansami_exe/Stellar_Classification_Capstone.git)
   cd Stellar_Classification_Capstone
   ```
2. **Set Up Environment:**

   ```bash
   # Create & activate virtual environment (adjust for your OS)
   python -m venv .venv
   source .venv/bin/activate # Or .\.venv\Scripts\activate on Windows

   # Install dependencies
   pip install -r requirements.txt
   ```

   *(**Note:** The raw data `star_classification.csv` and saved model `best_model.pkl` are included in the repository for immediate use).*
3. **Run the Streamlit App:**

   ```bash
   streamlit run app.py
   ```

   Navigate to the provided URL (e.g., `http://localhost:8501`) in your browser.
4. **(Optional) Re-run Training:**

   * To retrain all models and save the best one based on your data/environment:

   ```bash
   python run_pipeline.py
   ```

---

## ðŸ“‚ Project Structure

```
Stellar_Classification_Capstone/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py      # Data loading and validation
â”‚   â”‚   â”œâ”€â”€ data_transformation.py # Data cleaning and feature engineering
â”‚   â”‚   â””â”€â”€ model_trainer.py       # Model training and evaluation
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ train_pipeline.py      # Complete training workflow
â”‚   â”‚   â””â”€â”€ predict_pipeline.py    # Prediction pipeline
â”‚   â”œâ”€â”€ config.py                  # Configuration and constants
â”‚   â”œâ”€â”€ utils.py                   # Utility functions (plotting, logging, SHAP)
â”‚   â””â”€â”€ check_data.py              # Data quality validation (if applicable)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Raw SDSS data (e.g., star_classification.csv)
â”‚   â””â”€â”€ processed/                 # Processed datasets (train, val, test splits)
â”œâ”€â”€ saved_models/                  # Trained models and preprocessors (.pkl files)
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/                   # Saved plots and visualizations
â”‚   â”œâ”€â”€ final_model_summary.json   # A JSON file with the final model's summary
â”‚   â””â”€â”€ model_performance_comparison.csv # Model comparison results
â”œâ”€â”€ notebooks/                     # Jupyter notebooks for EDA and experimentation
â”œâ”€â”€ assets/                        # Static assets like images/GIFs
â”œâ”€â”€ docs/                          # Project documentation and reports
â”‚   â”œâ”€â”€ Description.docx           # Project description document
â”‚   â”œâ”€â”€ Stellar_Classification_Report.pptx  # Your PPT presentation file
â”œâ”€â”€ .gitignore                     # Specifies files/folders Git should ignore
â”œâ”€â”€ app.py                         # Streamlit web application script
â”œâ”€â”€ LICENSE                        # Project license file (e.g., MIT)
â”œâ”€â”€ README.md                      # Project overview and instructions
â”œâ”€â”€ requirements.txt               # Python package dependencies
â””â”€â”€ run_pipeline.py                # Script to execute the full training pipeline
---
```
## ðŸ“Š Results

The **Random Forest Classifier** (within the Scaler -> SMOTE -> Classifier pipeline) emerged as the top performer, achieving:

* **Test Accuracy:** ~97.7%
* **Test F1 (Macro):** ~0.973

Key features driving predictions include `redshift`, color indices (like `u_g`, `g_r`), and magnitude standard deviation (`mag_std`), as confirmed by SHAP analysis.

---

## ðŸ“œ License

This project is licensed under the MIT License. See the [LICENSE] file for details.

---

**Happy Classifying! ðŸŒŒ**
```
